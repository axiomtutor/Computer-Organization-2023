---
title: "A Companion to Computer Organization"
subtitle: "Measuring Performance"
author: "by Adam Frank"
# chalkboard: 
#     src: chalkboard.json
format: 
    revealjs: 
        transition: slide
        transition-speed: slow
        auto-slide: false # 10000
        theme: night
        smaller: true
        scrollable: true
        toc: true
slide-level: 0
controls: true
controls-layout: bottom-right
# slide-tone: true
incremental: true
overflow: auto
to: revealjs
---

# A Companion to Computer Organization

## by Adam Frank

### Measuring Performance

::: notes
Welcome to a video series companion to computer organization. In this video we will continue looking at the first chapter of the textbook, sections 1.4 to 1.6, which are thematically on the topic of performance.
:::

------------------------------------------------------------------------

# Speed of a computer

## Performance and Response Time

-   One way to measure the speed of a computer is the time that it takes to complete a task. This is called the computer's **response time**.
-   If you have a computer, run a program, and then measure the time that it takes for this program to finish -- you have measured the response time.  
	- This can be used as a benchmark to compare one computer against another.  
	- As a benchmark it is not a very clear signal of speed, because different computers can handle different kinds of tasks better and worse.  
		- One computer with a better GPU than another, but an inferior CPU, might be able to run a game more smoothly while taking longer to run a program that makes significant use of recursion, for instance.  

------------------------------------------------------------------------

# Throughput

-   Another way to measure the speed of a computer is the total number of tasks that it performs in unit time.  
-   Imagine a server receiving many requests for different tasks.  If you set a clock, and then measure the number of requests that it's able to process in some fixed amount of time, this measures its throughput.  
-   If a server adds extra processors in order to handle more requests simultaneously, it can increase throughout without necessarily increasing response rate, because any one request may still get processed by some particular processor at the same speed.  

------------------------------------------------------------------------

# Comparing Performance

Suppose that computer $A$ has twice the response time as computer $B$.  We might describe this by the equation 

$$ 
T_A = 2T_B 
$$

which is equivalent to 

$$ 
\frac{T_A}{T_B} = 2 
$$

Since response time is the reciprocal of performance, if we say that performacne is $P_A$ and $P_B$ respectively, then 

$$ 
\frac 1 {P_A} = 2\frac 1 {P_B} 
$$ 

which is equivalent to either of the following

$$ 
\frac{P_B}{P_A} = 2, \quad P_B = 2P_A
$$

In short, if $A$ has twice the response time then $B$ has twice the performance.

------------------------------------------------------------------------

# Measurement

-   A single processor may be trying to perform computations for several different programs.

    -   Suppose there are three programs A, B, C. If the computer is trying to make all three simultaneously useful, then it may compute tasks from each in a cycle: A, B, C, A, B, C, ...

    -   In some cases, it can make more sense for the processor to give one program more frequent computations than another, so that instead it computes something more like A, C, A, B, C, A, A, B, ... 

    -   Therefore it is not easy to get a pure measurement of response time since the computer is not purely using its resources on a single task.  

-   It's not always clear to the user, which programs are running. Sometimes the processor is busy with moving memory around, sometimes the operating system is performing necessary functions, hidden to the user.

    -   Therefore one cannot fix this problem merely by shutting down all other visible programs before running the text -- there will often still be invisible tasks, and blocking them may make the computer unusable.

------------------------------------------------------------------------

# CPU Time

-   **CPU time** for a task, is the time that the CPU spends doing computations needed *only* for that task and not for anything else.

    -   But importantly, CPU time *does* include the time spent on the OS if those computations are on behalf of the task. This part of CPU time is called **system CPU time**.

    -   The part of CPU time spent only on computations directly requested by the task, is called the **user CPU time**. (The "user" is the task.)

-   "system performance" will refer to net elapsed time, and "user CPU time" for user CPU time. ðŸ˜›

------------------------------------------------------------------------

# The Processor Clock

-   Processors in fact work by running "pulses" of electricity through the chips.  Each pulse causes a sequence computations to take place inside, called a **clock cycle**.  The faster these pulses happen, the faster the computer runs.  

-   One limitation on the speed of the computer, is the fact that these pulses of electricity generate heat.  

	-   If the pulses take place too quickly and the chip cannot dissipate heat fast enough, then the chips can be damaged by the heat.  

-   The **clock rate** or of the computer is the time between these pulses, often in the range of 250 picoseconds.  

-   Suppose a task requires $n$ clock cycles to finish, and each clock cycle takes $t$ units of time.  Then the response time $e$ is related to $n$ and $t$ by

	$$
	e = nt
	$$

-   Since $t$ is the time per clock cycle, then its reciprocal $1/t$ is the number of cycles per unit time -- that is to say, $1/t$ is the clock rate.

------------------------------------------------------------------------

# Instruction Performance

-   If you want to compare two computers, it can make sense to see which one performs *instructions* faster -- after all, a program just is a sequence of instructions!

-   Therefore let us form an equation which uses the average clock cycles per instruction (**CPI**).  Let $n$ be the number of clock cycles performed during a task and $i$ the number of instructions requested by that task.  Then

    $$ \frac n i = (CPI) $$

-   This equation is equivalent to $$n = i (CPI)$$

    which is the version given in the textbook (but with different names for variables).

------------------------------------------------------------------------

# The Classic CPU Equation

-   We now combine the two equations from earlier.

    $$
    e=nt
    $$

    $$
    \frac n i = (CPI)
    $$

-   Both equations involve $n$ and therefore can be solved for $n$.

    $$
    n = \frac e t = i(CPI)
    $$

-   If you multiply the $t$ to the other side, you get the first equation in the textbook.

    $$
    e = it(CPI)
    $$

-   $t$ is the time per cycle. Its inverse is the **clock rate**, which I'll denote by *r*. Then

    $$
    e = \frac{i(CPI)}{r}
    $$

-   The value of this equation is that it is clear that we want to minimize $e$, and the equation shows that we can do so by

    -   Minimizing $i$, the number of instructions in the program. This is the job of writing good *algorithms*.

    -   Minimizing $(CPI)$, which is the job of making a good processors.

    -   $r$ is unimportant since it is the time per cycle. As such it's mostly just a unit of measurement of time. If you change the unit of measurement, you don't change the quantity that you're actually measuring.

        -   Of course you may wonder "But numerically, if you increase the number $r$ that means that $e$ decreases. So $r$ must actually matter, right?"

        -   If you increase $r$, the cycles per unit time, then you also increase $(CPI)$ the cycles per instruction. So these balance out.

------------------------------------------------------------------------

# Instruction Mix

The $(CPI)$ measurement of course involves an average -- the average number of cycles per instruction. However, there are many different kinds of instructions, some requiring more time to compute than others. And if you have two different programs, one of them may happen to require more of the slower instructions. The **instruction mix** of a program is the number of each kind of instruction it requires.

Therefore a more honest measurement would divide up the different kinds of instructions. One would want to count the number of each kind of instruction in a program, and the cycles required for each kind. With this one can compute a weighted average which gives a better representation of the number of cycles per instruction.

------------------------------------------------------------------------

# Power

Without going into the details: Circuits are made of transistors, and transistors are made of semiconductor materials. The amount of electrical power consumed by a computer circuit is determined by some physics that is not interesting for our current purposes -- but what you do need to know is that a given material (the semiconductor) has an intrinsic capacitive load.

Moreover, a voltage is applied to the circuit in order to switch a transistor. And of course a transistor may be switched repeatedly. If we write $P,C,V,f$ for the power consumed, capacitive load, voltage applied, and frequency of switching, then these are related by the equation

$$
P=CV^2 f
$$

------------------------------------------------------------------------